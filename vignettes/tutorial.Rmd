<!--
%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{An R Markdown Vignette with knitr}
-->
```{r setup, include=FALSE}
library(knitr)
library(PGMscore)
require(igraph)
library(ggplot2)
```

## PGMscore tutorial
A factorgraph consists of variables and factors. 

```{r echo=TRUE, warning=FALSE}
varDim <- rep(4,6)
facPot <- c(list(matrix(0.25,1,4)),
            list(matrix(0.25,4,4)),
            list(matrix(0.25,4,4)),
            list(matrix(0.25,4,4)),
            list(matrix(0.25,4,4)),
            list(matrix(0.25,4,4)))
facNbs <- c(list(c(1L)),
            list(c(1L,2L)),
            list(c(1L,3L)),
            list(c(3L,4L)),
            list(c(3L,5L)),
            list(c(5L,6L)) )
facNames = c("Prior",rep("Int",5))
varNames = c("Foo","Bar","Baz","Do","Re","Mi")
mydfg <- dfg(varDim, facPot, facNbs, varNames, facNames)
```

If you have `igraph` installed the structure of the graph can be plotted using `plot`
```{r}
plot(mydfg)
```

#### Tail approximation
Suppose we have two models with the same graph structure, but different factor potentials. We can define a score of an observation as the loglikelihoodratio
$$
S(x) = \log \frac{\prod_{\mathcal{A}}f_a^\prime(x_a)}{\prod_{\mathcal{A}}f_a(x_a)} = \sum_{\mathcal{A}} \left[\log f_a^\prime(x_a) - \log f_a(x_a)\right]
$$

Motivate by bayesian statistics...

Lets define a foreground model where the variables are correlated with the ancestors
```{r}
rndDist <- function(row,col){
  x <- rexp(row*col,1/(c(1:row)+2))
  mat <- matrix(x, row, col, byrow = T)
  mat/rowSums(mat)
}
set.seed(1)
facPotFg <- c(list(rndDist(1,4)),
              list(rndDist(4,4)),
              list(rndDist(4,4)),
              list(rndDist(4,4)),
              list(rndDist(4,4)),
              list(rndDist(4,4)))
```

Lets calculate the tail probabilities in different points using the saddlepoint approximation
```{r}
x <- seq(-3,4,0.01)
dfsaddle <- tailSaddle(x, mydfg, facPotFg)

library(ggplot2)
ggplot(dfsaddle, aes(x=x,y=p)) + geom_line() +
  theme_bw() + 
  scale_y_log10() +
  ylab("p-value") + 
  xlab("score") +
  ggtitle("Saddlepoint approximation of the upper tail")
```

Naive simulation of scores to give estimates of $P(S > x)$. Notice that confidence bands is narrow for small scores and wide for high scores. Most often our interest is the 
```{r}
dfnaive <- tailIS(x, n=1000, alpha=0, dfg=mydfg, facPotFg=facPotFg)
#Notice that taking alpha=0 corresponds to naive sampling

ggplot(dfnaive, aes(x=x,y=p)) + geom_line() +
  theme_bw() +
  scale_y_log10() + 
  geom_ribbon(aes(ymin=pmax(low,0.0001),ymax=high),alpha=0.3,fill="blue") +
  annotation_logticks(sides="l")
```

To get better estimates of $P(S > x)$ for higher values of $x$, we can apply importance sampling. The importance sampling distribution we use is defines by
$$
P^{IS}(x) \propto P^{bg}(x)^{1-\alpha}P^{fg}(x)^{\alpha}
$$
It is seen that putting $\alpha=0$ corresponds to naive sampling. We now estimate tail probabilities using importance sampling and $\alpha=1.5$.

```{r}
dfis <- tailIS(x, n=1000, alpha=1.5, dfg=mydfg, facPotFg=facPotFg)
#Notice that taking alpha=0 corresponds to naive sampling

ggplot(dfis, aes(x=x,y=p)) + geom_line() +
  theme_bw() +
  scale_y_log10() + 
  geom_ribbon(aes(ymin=pmax(low,0.0001),ymax=high),alpha=0.3,fill="red") +
  annotation_logticks(sides="l")
```

We see quite opposite to naive sampling, that we have wide confidence bands for small $x$ and more narrow confidence bands for large $x$. So far we don't have any general advise for picking a good value of $\alpha$, the smaller probabilities you need to estimate, the larger $\alpha$.

Lets compare the estimates from our three methods. The dashed line is the saddlepointapproximation, the blue line is naive sampling and in red importance sampling:

```{r, echo=FALSE}
ggplot(dfsaddle, aes(x=x)) + geom_line(aes(y=p),linetype="longdash") +
  geom_line(data=dfis, aes(y=p), colour="red") +
  geom_line(data=dfnaive, aes(y=p), colour="blue") +
  geom_ribbon(data=dfis, aes(ymin=pmax(low, 0.001),ymax=high),alpha=0.3,fill="red") +
  geom_ribbon(data=dfnaive, aes(ymin=pmax(low, 0.001),ymax=high),alpha=0.3,fill="blue") +
  theme_bw() + 
  scale_y_log10() +
  ylab("p-value") + 
  xlab("score") +
  ggtitle("Estimates of uppertail of score function") +
  annotation_logticks(sides="l")
```

##Combining
Different values of alpha can be employed to get a narrow confidence limits over a long range of scores

```{r}
dfis <- tailIS(x, n=1000, alpha=c(0,0.5,1,1.5), dfg=mydfg, facPotFg=facPotFg)
#Notice that taking alpha=0 corresponds to naive sampling

ggplot(dfis, aes(x=x,y=p)) + geom_line() +
  theme_bw() +
  scale_y_log10() + 
  geom_ribbon(aes(ymin=pmax(low,0.0001),ymax=high),alpha=0.3,fill="red") +
  annotation_logticks(sides="l")
```

##Quantiles
The sampling function can also be used to estimate quantiles. 

```{r}
dfnaive <- tailIS(q=seq(0.01,0.5,by=0.001), n=1000, alpha=0, dfg=mydfg, facPotFg=facPotFg)
#Notice that taking alpha=0 corresponds to naive sampling

ggplot(dfnaive, aes(x=p,y=x)) + geom_line() +
  theme_bw()
```
