---
layout: post
title: Normal Mixture
author: "Tobias Madsen"
tags: [example]
---



<!--
Build post
setwd("../gh-pages-dgRaph/")
library(knitr)
render_jekyll()
knit(input = "../gh-pages-dgRaph/knitr//normalMixture.Rmd", output = "../gh-pages-dgRaph/_posts/2015-06-11-Normal-Mixture.md")
setwd("../dgRaph/")
-->

### When will I use a mixture of Normals?
So, you have misplaced your sample labels and a histogram of your data looks like this:



![center](/dgRaph/figs/normalMixture/plotMixture-1.png) 


{% highlight r %}
head(data)
{% endhighlight %}



{% highlight text %}
##             x
## 532  8.999322
## 107 12.537784
## 775 18.458876
## 104  7.975999
## 450 11.366255
## 54  13.444432
{% endhighlight %}

Now you wonder: Which measurements were from the siberian tigers you encountered on your adventures in the far east, and which measurements were from domestic cats at you grandma's?

Modelling this as a mixture of normals, we will be able to answer questions such as: How big are cats and tigers? How many were there of each? Which observations where tigers whichs where cats?

And by the way the data is just simulated using

{% highlight r %}
set.seed(2)
data <- data.frame(x = c(rnorm(600, 10, 2.7), rnorm(300, 20, 2.7)))
data <- data[sample.int(900),,drop=F]
{% endhighlight %}

### Building a Model


Being a bit more formal: From now on we will call the status of either cat or tiger a class-label. We do not observe the the class label, instead we observe the size. The size has a different distribution for each class. A natural model will be.

$$
H \sim Bernoulli(p) \\\\
X \mid H = 0 \sim \mathcal{N}(\mu\_0, \sigma\_0^2) \\\\
X \mid H = 1 \sim \mathcal{N}(\mu\_1, \sigma\_1^2)
$$

So we imagine that observations are generated by the following process: Flip a (biased) coin to decide which class the observation comes from, then draw from a normal distribution where the mean and variance depends on the class.

![center](/dgRaph/figs/shared/mixture_fac.svg)

To build this model in `dgRaph` we first need to decide the variable dimensions. The first variable `H` has dimension 2, from the two classes. The second variable `X` is continuous, so we will have to discretize it, lets say we decide to discretize it into 100 bins. 


{% highlight r %}
varDim <- c(2, 100)
{% endhighlight %}

Second we need some factors(or really factor potentials). We will need a prior for `H`, tells us what is the proportion of samples from each class. To this end we can use a `multinomialPotential`. We also need the two normal distribution, that is conditional distributions given `H`, here we can use `normalPotential`. 


{% highlight r %}
facPot <- list(multinomialPotential(dim = c(1, 2)),
               normalPotential(dim = c(2, 100)))
{% endhighlight %}

The last (strictly necessary) element is a description of the factor graph structure: Which variables are neighbors to which factors? So the prior is the neighbor of `H` the first variable, so it's neighbors is `c(1)`. The normal distribution potentials has both `H` and `X` as neighbors, alas both first and second variable, the rule is that the first neighbor is the variable we condition on, `H`, the second neighbor is the variable which conditional distribution the potential desribe, `X`, so the neighbors are `c(1,2)`. I repeat *First neighbor is the variable we condition on, second neighbor is the variable which conditional distribution we describe*.


{% highlight r %}
facNbs <- list(c(1),
               c(1,2))
{% endhighlight %}

We can now build the model


{% highlight r %}
mixDfg <- dfg(varDim = varDim, facPot = facPot, facNbs = facNbs, varNames = c("H", "X"), facNames = c("Prior H", "X | H"))
plot(mixDfg)
{% endhighlight %}

![center](/dgRaph/figs/normalMixture/mixDfg-1.png) 


### Training

We have build the model, but have not yet estimated the parameters $p, \mu\_0, \mu\_1, \sigma\_0^2, \sigma\_1^2$.
This is done using the `train` function. But first we have to create a dataset in the format that the `dgRaph` functions expect. That is discrete data and a column for each variable even the hidden variables. If a variable is unobserved we just supply `NA`.


{% highlight r %}
library(dplyr)
data_discrete <- data %>%
  mutate(H = NA) %>%
  mutate(X = as.integer(cut(x, breaks = seq(0,30,length.out = 101), labels = c(1:100)))) %>%
  select(H, X)
head(data_discrete)
{% endhighlight %}



{% highlight text %}
##    H  X
## 1 NA 30
## 2 NA 42
## 3 NA 62
## 4 NA 27
## 5 NA 38
## 6 NA 45
{% endhighlight %}

To get some meaningful output we also have to register a training function


{% highlight r %}
optimFun <- list(mixNorm = normOptimize(range = c(0, 30)))
{% endhighlight %}


{% highlight r %}
train(data_discrete, mixDfg, optim = c("row", "mixNorm"), optimFun = optimFun, iter.max = 500)
{% endhighlight %}



{% highlight text %}
## Training...
## Iterations:.........................................................................................................
## EM-algorithm converged after 105 iterations
## Likelihood: -3772.363
{% endhighlight %}

![center](/dgRaph/figs/normalMixture/train-1.png) 

{% highlight text %}
## 1th potential
## Row normalized multinomial potential
## 	1	2
## 1	0.33488	0.66512
## 
## 2th potential
## norm-potential update
## 1	mean:	20.003	var:	7.6938
## 2	mean:	10.268	var:	7.9092
{% endhighlight %}

### Inference

Let's have a look at the distribution. Firstly as we could already see from training, the prior assigns 1/3 and 2/3 of weight to each class. To see the distribution of size with-in the two classes, we can use the `potentials` functions, which returns a list of potentials. We will have a closer look at the second potential.


{% highlight r %}
pot <- potentials(mixDfg)[[2]]
library(reshape2)
pot_df <- melt(pot)
pot_df$size <- seq(0, 30, length.out = 101)[pot_df$Var2]
pot_df$Var1 <- as.factor(pot_df$Var1)
ggplot(pot_df, aes(x = size, y = value, colour = as.factor(Var1))) + geom_line()
{% endhighlight %}

![center](/dgRaph/figs/normalMixture/potentialsPlot-1.png) 

Finally lets classify our observations. We can do this using the `mps` function(most probable state). This gives us a dataframe where the `NA` has been replaced by the most probable state.


{% highlight r %}
mixMps <- mps(data = data_discrete, dfg = mixDfg)
head(mixMps, 10)
{% endhighlight %}



{% highlight text %}
##    H  X
## 1  2 30
## 2  2 42
## 3  1 62
## 4  2 27
## 5  2 38
## 6  2 45
## 7  2 42
## 8  2 43
## 9  2 23
## 10 2 30
{% endhighlight %}

### Code

Download the full example [here]({{ site.baseurl}}/downloads/normalMixture.R).

### Beyond

It is easy to extend to more than just two classes. We also plan to extend from normal distributions to other classes, but at the moment we only support normal and beta distributions. Finally a user can supply his own optimization functions, we will cover that in another example. Stay tuned :)
